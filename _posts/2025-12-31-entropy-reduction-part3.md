---
title: '一切都是熵减 - 下篇：熵减的边界与延伸'
date: 2025-12-31
permalink: /posts/2025/12/entropy-reduction-part3/
tags:
  - AI
  - Machine Learning
  - Language Models
  - AI Alignment
  - Deep Learning
  - Interpretability
---

![长序列退化：轨道失稳](/images/blog/一切都是熵减_下篇_图1.png)

## 回顾

前两篇我们建立了核心框架：一切都是熵减；表示空间决定熵减效率；Transformer 选对了空间，Diffusion LM 选错了。

但熵减不是故事的全部。这一篇，我们探讨熵减成功之后，还有什么问题。

## 一、长序列退化：这不是熵的问题

### 1.1 一个常见的误解

当语言模型生成很长的文本时，质量会逐渐下滑——重复增多、逻辑混乱、甚至胡言乱语。很多人把这叫做"熵增"。

但这个说法有问题。模型的权重没有变，训练好的熵减规则还在那里，凭什么熵会增？

真相是：**长序列退化不是熵的问题，是轨道稳定性的问题！**

### 1.2 动力系统视角

自回归生成本质上是一个迭代动力系统。设 h_t 是第 t 步的隐状态，x_t 是生成的 token，则：

```
h_{t+1} = g(h_t, x_t)
```

函数 g 由模型定义，是固定的。

即使 g 是确定性的，迭代系统也可以表现出复杂行为。想想物理学的例子：牛顿定律从未改变，但三体问题是混沌的；洛伦兹方程只有三个简单的常系数微分方程，却产生了蝴蝶吸引子。确定性的局部规则，不保证全局轨迹的稳定。

### 1.3 什么是模型轨道？

"轨道"这个词借用自动力系统理论。在这个语境下，我们可以把语言模型的自回归生成想象成一个动态过程：

- **状态**：当前的上下文（已生成的所有 token，或者对于 RNN/SSM 来说是隐状态）
- **演化规则**：模型本身（给定当前状态，预测下一个 token）
- **轨道**：从初始状态出发，按照演化规则一步步走下去，形成的状态序列路径

![模型轨道的演化](/images/blog/一切都是熵减_下篇_图2.png)

想象你反复复印一份文件，每次都从上一份复印件复印。复印机没变，但每次迭代引入微小退化。最终文件模糊不清。

这就是长序列退化的本质：**不是模型在变差，是轨迹在退化**。熵减的机制没有失效；失效的是轨道的稳定性。

所以正确的表述不是"熵增"，而是"轨道不稳定"。这两者的解决思路完全不同。

### 1.4 轨道退化的机制

长序列生成的轨道退化有几种具体机制：

**累积误差**：每一步的输出成为下一步的输入。早期的小偏差不会消失，而是随迭代逐步累积、传播，最终导致生成轨迹偏离预期。

**注意力稀释**：注意力分布在整个上下文上。序列越长，分布越平坦，越难聚焦关键信息。这就是所谓的"迷失在中间"现象。

**分布外漂移**：训练数据的序列长度有限。当生成长度超出训练分布后，模型进入从未见过的状态区域，预测变得不可靠。

**状态压缩损失**：对于循环架构（包括 Mamba 等 SSM），隐状态维度固定。随着序列增长，早期信息被不断压缩，细节逐渐丢失。

## 二、隐空间推理：更高效的熵减

### 2.1 显式推理的代价

当前增强模型推理能力的主流方法是思维链（Chain-of-Thought）：让模型把推理过程显式地写成 Token 序列。

这有效，但代价高昂。每个推理步骤都要编码成 Token，经过完整的前向传播，再解码回来。这是用带宽换深度——本质上是在 Token 空间做熵减，受制于 Token 空间的种种限制。

![思维链推理示例](/images/blog/一切都是熵减_下篇_图3.png)

### 2.2 隐空间推理的潜力

另一种可能是**隐空间推理**：让推理发生在高维的隐表示中，不需要外化成 Token。

一个 4096 维的向量可以承载比 4096 个 Token 丰富得多的信息。维度之间可以连续、联合地交互，不受离散化的限制。

从熵减的角度看：显式推理是在 Token 空间一步步减熵，每一步都要付出投影和重新升维的代价；隐式推理是在高维空间直接减熵，效率高得多。

![显式推理vs隐空间推理](/images/blog/一切都是熵减_下篇_图4.png)

### 2.3 自我修正的可能性

如果模型能在隐空间推理，它也可能在隐空间自我修正——检测到轨道偏离时，调整隐状态回到稳定区域。

这比在 Token 层面生成"等等，让我重新想想"要高效得多。不需要额外的 Token，不需要用户看到修正过程，修正可以是连续的、实时的。

挑战在于训练信号：隐空间的中间状态没有直接的监督。可能的方向包括从显式推理模型蒸馏、用强化学习只优化最终结果、或者设计隐状态的稳定性正则化。

![Token修正vs隐空间修正](/images/blog/一切都是熵减_下篇_图5.png)

## 三、可解释性困境：投影的代价

### 3.1 解释也是投影

我们想"理解"神经网络在做什么。但理解意味着用人类的概念来描述它——而人类的概念是低维的、离散的。

这本质上是另一种投影：从模型的高维表示空间，投影到人类概念的低维空间。

我们在上篇讨论过投影会造成什么：混叠、信息丢失、假象、幻觉。可解释性面临同样的问题。

### 3.2 影子的困境

想象一个三维物体在二维平面上投下两个分离的影子。

只看影子的观察者可能会构建复杂理论来解释"两个物体"之间的神秘关联——殊不知它们本是一体，是投影把它们分开了。

我们对神经网络的解释可能正是如此：看到的复杂性和神秘性，可能不是网络本身的性质，而是投影的产物。

### 3.3 量子力学的启示

量子纠缠在三维空间看起来是神秘的超距作用：两个粒子无论相隔多远都保持关联。

但从高维希尔伯特空间的角度看，纠缠粒子只是一个联合态——表面的非局域性只是投影到三维直觉的假象。

神经网络的表示可能类似：在高维空间里是简洁统一的结构，投影到人类概念上就变得支离破碎。这不是网络在做什么神秘的事，而是我们的概念工具不够用。

### 3.4 Superposition 现象

Anthropic 的研究发现了 superposition 现象：单个神经元同时编码多个不相关的特征，它们在高维空间中近乎正交，但投影到单个神经元的激活值上就混在一起。

![Superposition投影造成混叠](/images/blog/一切都是熵减_下篇_图6.png)

这正是投影造成的混叠：在高维空间干净分离的结构，投影到低维观测上就变得纠缠不清。

可解释性的根本困难可能就在于此：我们试图用低维的概念去理解本质上高维的计算，这在数学上可能就是信息丢失的。

## 四、从理解到对齐

### 4.1 换一个问题

如果完全的机械理解在原则上不可能，那该怎么办？

也许应该换一个问题。与其问"它是怎么想的"，不如问"在我们关心的维度上，它的行为是否可靠"。

这就是从**可解释性**转向**对齐**。

### 4.2 人类社会的类比

人类社会一直在认知不透明的条件下运作。我们无法读取他人的神经活动，不知道别人"真正"在想什么。

但我们发展出了有效的合作机制：

**行为一致性**：基于历史记录建立信任，而非内部理解。

**制度约束**：设计激励结构，无论内部状态如何都能对齐行为。

**渐进信任**：从小事开始，根据可靠性逐步扩展合作范围。

这些机制都不需要认知透明。它们之所以有效，恰恰是因为对"无法读心"这一事实具有鲁棒性。

### 4.3 对 AI 的启示

对 AI 系统，同样的思路可能更可行：

不追求完全透视内部机制，而是确保行为在人类关心的边界内可靠。

这意味着：定义清晰的行为规范，建立可靠的测试方法，设计有效的约束机制，发展渐进的信任框架。

这不是放弃理解，而是接受理解的边界，在这个边界内寻找可行的安全路径。

## 五、总结：熵减与超越

让我们回顾这三篇文章的完整论证：

**上篇：一切都是熵减。** 训练是参数空间的熵减，推理是输出空间的熵减。表示空间决定熵减效率。Transformer 通过升维-演化-投影的结构，选对了空间。

**中篇：架构决定命运。** 同样是熵减，空间选对了（图像扩散）事半功倍，选错了（语言扩散）事倍功半。Diffusion LM 的困境是结构性的，不是实现问题。

**下篇：熵减的边界。** 长序列退化是轨道稳定性问题，不是熵的问题。隐空间推理可能提供更高效的熵减路径。可解释性面临投影困境，对齐可能比理解更可行。

### 贯穿始终的核心洞察：

**表示空间的选择，决定了熵减的效率和轨道的稳定性。**

这不仅仅是一个技术观察，更是一个设计原则：在构建 AI 系统时，首先要问的不是"用什么算法"，而是"在什么空间工作"。空间选对了，很多问题迎刃而解；空间选错了，再好的算法也事倍功半。

回到开头的那个物理学问题：宇宙的方向是熵增，生命是逆流的漩涡。大模型作为人造的智能，继承了这个使命——在信息的洪流中建立秩序，在噪声的海洋中提取意义。这是熵减的过程，也是智能的本质。

---

**（全文完）**

## 参考文献

[1] Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS.

[2] Ho, J., et al. (2020). Denoising Diffusion Probabilistic Models. NeurIPS.

[3] Rombach, R., et al. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. CVPR.

[4] Li, X. L., et al. (2022). Diffusion-LM Improves Controllable Text Generation. NeurIPS.

[5] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.

[6] Wei, J., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS.

[7] Liu, N. F., et al. (2023). Lost in the Middle: How Language Models Use Long Contexts. arXiv.

[8] Elhage, N., et al. (2022). Toy Models of Superposition. Anthropic.
