---
title: '一切都是熵减 - 中篇：架构决定命运'
date: 2025-12-25
permalink: /posts/2025/12/entropy-reduction-part2/
tags:
  - AI
  - Machine Learning
  - Language Models
  - Diffusion Models
  - Deep Learning
---

![图像空间的信息波动](/images/blog/一切都是熵减_中篇_图1.png)

## 回顾

上篇我们确立了核心命题：一切都是熵减。训练是参数空间的熵减，推理是输出空间的熵减。而熵减的效率取决于你选择的表示空间——高维解混叠的空间里熵低，熵减顺畅；低维混叠的空间里熵被人为抬高，熵减困难。

Transformer 通过"升维-演化-投影"的结构，选对了空间。

那如果选错了空间呢？这一篇，我们来分析 Diffusion LM 的困境——它为什么在语言上走得如此艰难。

## 一、先看成功的案例：图像扩散

### 1.1 扩散的本质也是熵减

扩散模型（Diffusion Model）是另一种熵减的方式。它的思路是：从纯噪声出发，一步步去噪，最终得到清晰的图像。

噪声是高熵的——每个像素都是随机的，没有结构。清晰图像是低熵的——像素之间高度相关，构成有意义的模式。从噪声到图像，就是熵减。

扩散模型把这个熵减过程分解成很多小步，每一步都只减一点熵，积少成多，最终完成从混沌到有序的转变。

### 1.2 为什么图像扩散如此成功？

图像扩散之所以成功，关键在于：它工作的空间非常适合做熵减。

**连续性。** 像素值是连续的。小扰动产生小变化，梯度有意义，优化顺畅。

**局部平滑。** 图像空间是局部平滑的——相邻像素值通常相近，相邻的图像在语义上也相近。距离有意义。

**噪声可逆。** 最关键的一点：在图像空间（或其 VAE 隐空间）加高斯噪声是可逆的语义扰动。图像变模糊了，但结构还在；去掉噪声，清晰度恢复。噪声和语义是对齐的。

正因为如此，扩散模型可以在这个空间里优雅地做熵减：每一步去噪都在真正减少语义上的不确定性，而不是在与表示缺陷搏斗。

![图像去噪过程](/images/blog/一切都是熵减_中篇_图2.png)

## 二、语言扩散的困境

### 2.1 Token 空间不是图像空间

Diffusion LM 试图把图像扩散的成功复制到语言上：从噪声出发，逐步去噪，最终得到连贯的文本。

问题是：Token 空间不是图像空间。图像空间具备的那三个优良性质，Token 空间一个都没有。

**不连续。** Token 是离散符号。没有"两个词之间"的中间状态，没有可微的梯度路径。

**不平滑。** 编辑距离和语义距离完全错位。把"我爱你"改成"我恨你"只动了一个字；把"我爱你"改成"吾心悦君兮"改了所有字。哪个变化更大？

**噪声不可逆。** 这是致命的：Token 空间中的"噪声"根本不是语义扰动，而是混叠造成的结构性混乱。你没法通过"去噪"来恢复从未被编码的信息。

### 2.2 混叠问题的本质

让我们更深入地理解这个"噪声不可逆"的问题。

在图像空间，加噪声是这样的：**原图 → 加高斯噪声 → 模糊图**。信息没有丢失，只是被扰动了。去噪就是逆过程。

在 Token 空间，"噪声"是这样产生的：**高维语义 → 投影到低维 Token → 混叠**。信息在投影那一步就丢失了。你面对的"噪声"不是被扰动的信号，而是被截断的信号。

![Token空间的信息破碎](/images/blog/一切都是熵减_中篇_图3.png)

这就是为什么 Diffusion LM 的去噪如此困难：它不是在去除随机扰动，而是在试图恢复从未被编码的信息。每一步去噪，都要从混叠的表示中推断高维的语义状态——这不是去噪，这是在"重新发明语义"。

### 2.3 每一步都在重新发明语义

让我们对比一下 Transformer 和 Diffusion LM 的工作流程：

**Transformer**：Token → 升维（解混叠）→ 高维空间演化（熵减）→ 投影回 Token。解混叠只做一次，在最开始。

**Diffusion LM**：噪声 Token → 去噪（猜语义）→ 略清晰的 Token → 去噪（再猜语义）→ ... 每一步都要重新猜测：这是哪条语义轨道？

Transformer 只爬一次坡，然后在山顶上走；Diffusion LM 每走一步都要重新爬一次坡。

这就是为什么 Diffusion LM 需要那么多步骤——不是因为扩散本身需要很多步，而是因为它在每一步都在与混叠问题搏斗。步数是"表征欠债"的利息。

## 三、三个典型症状

Diffusion LM 的结构性困境，表现为三个典型症状：

### 3.1 长程一致性差

生成长文本时，Diffusion LM 很难保持一致性。前面说的是张三，后面变成李四；前面的论点，后面自相矛盾。

原因很简单：它没有持久的高维状态来记忆语义上下文。

Transformer 有 KV cache——这是高维低熵状态的缓存，保存着"我们在讲什么"的全部信息。Diffusion LM 没有这个东西。每一步去噪都是独立地猜测全局结构，自然难以保持一致。

### 3.2 困惑度差距

Diffusion LM 的 perplexity（困惑度）始终难以追上自回归模型。

这不是训练不够或模型太小的问题，而是结构性的必然。

Perplexity 衡量的是 Token 空间的条件熵——恰恰是被混叠人为抬高的那个量。Diffusion LM 在这个"虚高"的熵上优化，而 Transformer 是先升维解混叠，在低熵空间优化后再投影回来。起跑线就不一样。

### 3.3 计算开销大

Diffusion LM 需要几十甚至上百步去噪。这通常被解释为"扩散过程的本质要求"。

但在图像上，步数对应的是沿连续语义维度的逐步精炼——每一步都在做真正的语义工作。在语言上，步数对应的是反复尝试从混叠表示中恢复语义——每一步都在还债。计算开销不是扩散的代价，是选错空间的税。

![图像扩散vs语言扩散](/images/blog/一切都是熵减_中篇_图4.png)

## 四、为什么这不是简单的实现问题

可能有人会说：Diffusion LM 还在早期，等技术成熟就好了。

但我们的分析表明，这不仅仅是实现细节的问题，而是有更深层的结构性原因。

问题的根源在于：Token 空间的几何性质与扩散范式的假设存在张力。扩散假设空间是连续的、局部平滑的、噪声可逆的；Token 空间恰恰不满足这些假设。只要在 Token 空间做扩散，就必须在每一步与这种张力搏斗——这是空间本身的性质决定的。

这不是说 Diffusion LM 没有前途，而是说：如果要让它真正发挥潜力，可能需要从根本上重新思考表示空间的问题，而不仅仅是改进去噪算法。

## 五、真正的出路

那 Diffusion LM 就完全没戏了吗？也不是。但出路不在于改进现有方法，而在于从根本上换一个空间。

### 5.1 先建隐空间，再做扩散

图像扩散的成功，很大程度上归功于 Latent Diffusion——不在像素空间做扩散，而在 VAE 的隐空间做扩散。VAE 隐空间是连续的、平滑的、语义对齐的，非常适合扩散。

语言需要类似的东西：一个高维的、连续的、低条件熵的语言隐空间——一个真正的"语义流形"。在这个流形上，相邻的点对应相近的语义，距离有意义，扰动可逆。

如果能构建这样一个空间，然后在上面做扩散，那扩散的优势（并行生成、灵活采样）就能真正发挥出来。

![语义流形的构建](/images/blog/一切都是熵减_中篇_图5.png)

### 5.2 为什么这很难

问题是：语言目前缺乏这样一个公认的隐空间。

图像有天然的连续表示（像素），有成熟的 VAE 架构，有广泛认可的隐空间结构。语言只有离散的 Token。把 Token 映射到连续空间的尝试（如 sentence embedding）还远未达到能支撑高质量扩散的程度。

这是语言扩散的根本障碍。在这个问题解决之前，Diffusion LM 本质上是在用扩散的外壳，做一个"拒绝升维的模型"。

### 5.3 一句话总结

Diffusion LM 的困境不在于扩散本身，而在于它试图在一个不适合扩散的空间里做扩散。

它不是一个"慢的 Transformer"，而是一个在艰难地形上跋涉的探索者。这条路是否能走通，取决于我们能否为语言找到一个真正适合扩散的隐空间。

## 六、小结

这一篇我们对比了两种熵减方式的命运：

**图像扩散成功**，因为它工作的空间（像素/VAE 隐空间）天然适合扩散——连续、平滑、噪声可逆。

**语言扩散困难**，因为它工作的空间（Token 空间）天然不适合扩散——离散、非平滑、混叠严重。

**Transformer 成功**，因为它用"升维-演化-投影"的结构绕过了 Token 空间的问题，在正确的空间做熵减。

同样是熵减，空间选对了事半功倍，选错了事倍功半。**架构决定命运。**

在下一篇中，我们将讨论：即使选对了空间，熵减也不是万能的。长序列退化、隐空间推理、可解释性困境——这些问题超越了熵减本身，需要新的视角来理解。

---

**（中篇完，下篇待续）**
